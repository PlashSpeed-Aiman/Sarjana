{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DNDT\\Documents\\venv\\sarjana-env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "text_splitter = CharacterTextSplitter()\n",
    "\n",
    "def split_the_pdf_texts(file_path: str) -> list[str]:\n",
    "    with open(file_path) as f:\n",
    "        how_to_win_friends = f.read()\n",
    "        texts = text_splitter.split_text(how_to_win_friends)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from llama_index.core import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "def load_pdf(path: Path) -> list[Document]:\n",
    "        pdf_loader = PyPDFLoader(path)\n",
    "        documents = pdf_loader.load_and_split()\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "docs = load_pdf(\"data/llama2.pdf\")\n",
    "print(type(docs))\n",
    "print(type(docs[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map reduce method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap\n",
    "\n",
    "def progressive_summarize(docs: list[Document]):\n",
    "    # with a custom prompt\n",
    "    prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\n",
    "    {text}\n",
    "\n",
    "    CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
    "\n",
    "    PROMPT = PromptTemplate(template=prompt_template,input_variables=[\"text\"])\n",
    "    ## with intermediate steps\n",
    "    chain = load_summarize_chain(OpenAI(temperature=0),chain_type=\"map_reduce\",return_intermediate_steps=True,map_prompt=PROMPT,combine_prompt=PROMPT)\n",
    "\n",
    "    output_summary = chain({\"input_documents\": docs}, return_only_outputs=True)\n",
    "    wrapped_text = textwrap.fill(output_summary['output_text'],width=100,break_long_words=False,replace_whitespace=False)\n",
    "    print(wrapped_text)\n",
    "    return output_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "- Llama 2-Chat is a collection of pretrained and fine-tuned large language models optimized for\n",
      "dialogue use cases.\n",
      "- It outperforms open-source chat models on most benchmarks and has been\n",
      "evaluated for helpfulness and safety.\n",
      "- The authors provide a detailed description of their approach\n",
      "to fine-tuning and safety improvements.\n",
      "- The goal is to enable the community to build on their work\n",
      "and contribute to the responsible development of LLMs.\n",
      "- The paper discusses the pretraining, fine-\n",
      "tuning, and safety measures used in the development of Llama 2 and Llama 2-Chat.\n",
      "- The models were\n",
      "evaluated on various benchmarks and showed strong performance.\n",
      "- Safety measures were implemented in\n",
      "both the pretraining and fine-tuning stages.\n",
      "- The paper also shares observations and insights from\n",
      "the development process.\n",
      "- A responsible release strategy and code examples are provided for safe\n",
      "deployment.\n",
      "- The study compares two reinforcement learning algorithms and explores the use of Ghost\n",
      "Attention for dialogue control over multiple turns.\n",
      "- The final system message for training is\n",
      "constructed by randomly combining synthetic constraints.\n",
      "- Llama 2-Chat has been evaluated for\n",
      "helpfulness and safety, with high inter-rater reliability and low overall violation percentage.\n",
      "-\n",
      "Researchers have also studied moral self-correction in large language models\n"
     ]
    }
   ],
   "source": [
    "summarized = progressive_summarize(docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intermediate_steps': ['\\n- Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs).\\n- The Llama 2-Chat models are optimized for dialogue use cases.\\n- Llama 2-Chat outperforms open-source chat models on most benchmarks.\\n- The models have been evaluated for helpfulness and safety and may be a suitable substitute for closed-source models.\\n- The authors provide a detailed description of their approach to fine-tuning and safety improvements.\\n- The goal is to enable the community to build on their work and contribute to the responsible development of LLMs.',\n",
       "  ' \\n\\n• The paper discusses the pretraining, fine-tuning, and safety measures used in the development of the Llama 2 chatbot.\\n• Pretraining involved using large amounts of data and training details to create a pretrained model.\\n• The pretrained model was evaluated using the Llama 2 Pretrained Model Evaluation method.\\n• Fine-tuning was done through supervised fine-tuning and reinforcement learning with human feedback.\\n• A system message for multi-turn consistency was used in the fine-tuning process.\\n• Safety measures were implemented in both the pretraining and fine-tuning stages.\\n• Red teaming was used to test the safety of the chatbot.\\n• The paper discusses the learnings, limitations, and ethical considerations of the development process.\\n• A responsible release strategy was implemented for the chatbot.\\n• Related work in the field is also discussed.\\n• The paper concludes with a summary of the contributions and additional details on the pretraining, fine-tuning, and safety measures used.',\n",
       "  ' \\n- Llama 2-Chat model was evaluated by human raters on ~4k prompts consisting of single and multi-turn prompts.\\n- The 95% confidence intervals for this evaluation are between 1% and 2%.\\n- Human evaluations can be noisy due to limitations of the prompt set, subjectivity of review guidelines and individual raters, and difficulty of comparing generations.\\n- Llama 2-Chat performed better than other open-source models in terms of helpfulness and safety.\\n- Llama 2-Chat was on par with some closed-source models in human evaluations.\\n- Measures were taken to increase the safety of Llama 2-Chat models, including safety-specific data annotation and tuning, red-teaming, and iterative evaluations.\\n- The paper also shares observations made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.',\n",
       "  ' \\n- Figure 3 shows safety evaluation results for Llama 2-Chat compared to other models.\\n- Human raters judged model generations for safety violations across ~2,000 adversarial prompts.\\n- Safety results should be taken with caution due to limitations of the prompt set and subjectivity of raters.\\n- Llama 2 and Llama 2-Chat are being released for research and commercial use.\\n- Llama 2 is an updated version of Llama 1, trained on new data and with increased pretraining corpus.\\n- Llama 2-Chat is a fine-tuned version optimized for dialogue use cases.\\n- Open release of LLMs can be beneficial but carries potential risks.\\n- Developers should perform safety testing and tuning tailored to their specific applications.\\n- A responsible use guide and code examples are provided for safe deployment.\\n- The paper covers pretraining methodology, fine-tuning methodology, model safety, observations and insights, related work, and conclusions.',\n",
       "  \"\\n• Figure 4 shows the training process of Llama 2-Chat.\\n• The process begins with pretraining Llama 2 using publicly available online sources.\\n• An initial version of Llama 2-Chat is created through supervised fine-tuning.\\n• The model is then refined using Reinforcement Learning with Human Feedback (RLHF) methodologies.\\n• The accumulation of iterative reward modeling data is crucial during the RLHF stage.\\n• The new Llama 2 models have improved performance compared to Llama 1 models.\\n• The training corpus includes a mix of data from publicly available sources, excluding data from Meta's products or services.\\n• Various pretraining data investigations were performed to understand the capabilities and limitations of the models.\\n• The model architecture is based on the standard transformer architecture with some modifications.\\n• Hyperparameters such as optimizer, learning rate schedule, weight decay, and gradient clipping were used during training.\",\n",
       "  \" \\n- Llama 2 is a family of models trained on a mix of publicly available online data.\\n- The models range in size from 7B to 70B tokens and are trained with a global batch-size of 4M tokens.\\n- The bigger models (34B and 70B) use Grouped-Query Attention (GQA) for improved inference scalability.\\n- The models are trained using the same tokenizer as Llama 1, with a vocabulary size of 32k tokens.\\n- The models were pre-trained on 2T tokens and did not show any sign of saturation.\\n- Training was done on Meta's Research SuperCluster (RSC) and internal production clusters, both using NVIDIA A100s.\\n- RSC uses NVIDIA Quantum InfiniBand while the production cluster uses RoCE (RDMA over converged Ethernet).\\n- The models were compared to assess the suitability of different types of interconnect for large-scale training.\",\n",
       "  \"\\n\\n- Llama 2 models were pre-trained using a total of 3.3M GPU hours, resulting in a carbon emission of 539 tCO2eq.\\n- The carbon emissions were directly offset by Meta's sustainability program and will not need to be incurred by other companies due to the open release strategy.\\n- The Llama 2 models were evaluated on various benchmarks, including code, commonsense reasoning, world knowledge, reading comprehension, and math.\\n- The results showed strong performance across all benchmarks, with 7-shot results for CommonSenseQA and 0-shot results for other benchmarks.\\n- The overall performance of Llama 2 models was comparable to expensive Infiniband systems, making pre-training more accessible.\",\n",
       "  ' \\n\\n- Llama 2 models outperform Llama 1 models on academic benchmarks.\\n- Llama 2 70B model outperforms all open-source models.\\n- Llama 2 70B is close to GPT-3.5 on MMLU and GSM8K benchmarks.\\n- Llama 2 70B results are on par or better than PaLM (540B) on most benchmarks.\\n- There is still a large gap in performance between Llama 2 70B and GPT-4 and PaLM-2-L.\\n- Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques.\\n- Fine-tuning, reward modeling, and RLHF techniques were used to improve Llama 2-Chat.\\n- A new technique, Ghost Attention, was found to help control dialogue flow over multiple turns.\\n- Safety evaluations on fine-tuned models are discussed in Section 4.2.',\n",
       "  '\\n• SFT (Supervised Fine-Tuning) is a stage in model training that uses publicly available data to improve performance.\\n• Quality is important in SFT data, so we collected high-quality examples through vendor-based annotation efforts.\\n• Different annotation platforms and vendors can result in different model performance, so data checks are important.\\n• For supervised fine-tuning, a cosine learning rate schedule is used with specific parameters.\\n• Each sample in fine-tuning consists of a prompt and an answer, with a special token used to separate them.\\n• An autoregressive objective is utilized and the loss is only backpropagated on answer tokens.\\n• The model is fine-tuned for 2 epochs.\\n• RLHF (Reinforcement Learning with Human Feedback) is a model training procedure that further aligns model behavior with human preferences and instruction following.\\n• Data is collected to represent human preferences and instruction following, and the model is trained to improve in these areas.',\n",
       "  '\\n- Human annotators select preferred model outputs\\n- Feedback used to train reward model\\n- Binary comparison protocol used for diversity\\n- Annotators choose between two model responses\\n- Responses sampled from different model variants and temperature hyper-parameter\\n- Annotators also label degree of preference\\n- Focus on helpfulness and safety\\n- Safety guidelines provided for annotators\\n- Safety label collected during safety stage\\n- Human annotations collected weekly\\n- Reward models improved with more data\\n- New data collected before each tuning iteration to maintain accuracy\\n- Large dataset of over 1 million binary comparisons collected\\n- Longer and more conversation turns compared to existing datasets\\n- Reward modeling used to automate preference decisions',\n",
       "  ' \\n- Preference data has more conversation turns and is longer on average\\n- The reward model takes a model response and prompt as inputs and outputs a score for quality\\n- This score is used to optimize Llama 2-Chat for better human preference alignment and improved helpfulness and safety\\n- Two separate reward models are trained for helpfulness and safety, as they sometimes trade off\\n- The reward models are initialized from pretrained chat model checkpoints to benefit from pretraining knowledge.',\n",
       "  ' \\n\\n- Dataset contains human preference data for reward modeling\\n- Includes open-source and internally collected data\\n- Each example consists of a prompt and response\\n- Binary ranking loss used for training reward model\\n- Margin component added to loss for better accuracy\\n- Data composition includes open-source datasets for better generalization and to prevent reward hacking\\n- Different mixing recipes experimented for best results.',\n",
       "  ' \\n- Helpfulness reward model is trained on all Meta Helpfulness data, combined with equal parts of remaining data sampled from Meta Safety and open-source datasets.\\n- Meta Safety reward model is trained on all Meta Safety and Anthropic Harmless data, mixed with Meta Helpfulness and open-source helpfulness data in a 90/10 proportion.\\n- Training is done for one epoch over the training data, with a warm-up of 3% of total steps and a minimum of 5.\\n- The maximum learning rate is 5x10^-6 for Llama 2-Chat and 1x10^-5 for the rest, with a cosine learning rate schedule.\\n- Effective batch size is fixed at 512 pairs or 1024 rows per batch.\\n- Final helpfulness and safety reward models outperform baselines on a diverse set of human preference benchmarks.\\n- Reward models show superior accuracy on more distinct responses and lower accuracy on similar responses.\\n- GPT-4 performs better than other non-Meta reward models, despite not being trained directly for this task.',\n",
       "  \"\\n\\n• Reward model accuracy improves with more data and larger model sizes.\\n• The tension between helpfulness and safety may affect the reward model's performance.\\n• Accuracy is higher for more distinct responses compared to similar ones.\\n• Iterative fine-tuning with PPO and rejection sampling methods was used to train successive versions of RLHF models.\\n• Rejection sampling involves sampling multiple outputs and selecting the best one based on the reward.\\n• The reward model is an important factor in improving Llama 2-Chat performance.\\n• Further improvements can be made with more data annotations.\",\n",
       "  ' \\n- The study compares two reinforcement learning (RL) algorithms, Rejection Sampling and PPO, for generating responses to prompts.\\n- Rejection Sampling explores multiple samples for a prompt, while PPO only generates one.\\n- The model is fine-tuned on the highest scoring samples, similar to previous studies.\\n- The two algorithms differ in breadth and depth, but the differences become less pronounced with iterative model updates.\\n- The study uses a combination of Rejection Sampling and PPO for better results.\\n- Rejection Sampling is only used with the largest model, and smaller models are fine-tuned on the sampled data.\\n- The study also explores the effect of distilling the capabilities of the larger model into smaller ones.\\n- At each iteration, the model samples multiple answers for a prompt and selects the best one based on the reward model.\\n- Previous versions of the model only used samples from the preceding iteration, but this led to limited improvement.\\n- The study shows that using samples from previous iterations can lead to better results.',\n",
       "  '\\n- Regression in some capabilities, specifically composing rhyming lines in poems, was observed in RLHFV3 compared to previous versions.\\n- Further investigation into the causes and mitigations for forgetting is suggested for future research.\\n- Modifications were made to the strategy, incorporating top-performing samples from prior iterations.\\n- The adjustments resulted in considerable enhancements in performance and addressed previous issues.\\n- Rejection Sampling was used to illustrate the benefit of fine-tuning on the best output.\\n- The temperature parameter plays a role in exploration, with higher temperatures allowing for more diverse outputs.\\n- The optimal temperature for Llama 2-Chat -RLHF was found to be between 1.2 and 1.3.\\n- The language model was trained using the RL scheme of Stiennon et al. (2020).\\n- The final reward function used during optimization includes a penalty term for diverging from the original policy.\\n- The safety and helpfulness reward models were combined to create the final reward function.\\n- The AdamW optimizer was used with specific parameters for all models.',\n",
       "  ' \\n- Weight decay of 0.1 is used\\n- Gradient clipping of 1.0 is used\\n- Constant learning rate of 10^-6 is used\\n- Batch size of 512 is used for each PPO iteration\\n- PPO clip threshold of 0.2 is used\\n- Mini-batch size of 64 is used\\n- One gradient step is taken per mini-batch\\n- For 7B and 13B models, β is set to 0.01 (KL penalty)\\n- For 34B and 70B models, β is set to 0.005',\n",
       "  '\\n• Figure 9 shows issues with multi-turn memory on the left and how they can be improved with GAtt on the right.\\n• Training for all models is done for 200-400 iterations, with early stopping based on evaluations on held-out prompts.\\n• Each iteration of PPO on the 70B model takes approximately 330 seconds.\\n• FSDP is used to train quickly with large batch sizes, but caused a slowdown during generation.\\n• Ghost Attention (GAtt) is proposed as a simple method to enable dialogue control over multiple turns.\\n• GAtt involves concatenating an instruction to all user messages in a dialogue dataset and using it to fine-tune the model.\\n• Synthetic constraints such as Hobbies, Language, and PublicFigure are used to create diverse instructions for training.\\n• The final system message for training is constructed by randomly combining these constraints.',\n",
       "  ' \\n- Modified original instruction to be less verbose, e.g. \"Always act as Napoleon from now\" -> \"Figure: Napoleon.\"\\n- Produced an SFT dataset and fine-tuned Llama 2-Chat using GAtt evaluation.\\n- Applied GAtt after RLHF V3 and found it to be consistent up to 20+ turns.\\n- Tried setting constraints not present in the training of GAtt at inference time, such as \"Always answer with Haiku,\" and found the model remained consistent.\\n- Displayed attention visualization for a dialogue with and without GAtt, showing that the GAtt-equipped model maintained large attention activations for a larger portion of the dialogue.\\n- Current implementation of GAtt is vanilla and further development and iteration could benefit the model.\\n- Evaluated LLMs using model-based evaluation and found improvement in rewards from latest reward models.\\n- Validated major model versions with human evaluations.\\n- Collected a test set of prompts for helpfulness and safety and had three annotators judge the quality of the models.',\n",
       "  \" \\n- Test set of prompts for helpfulness and safety\\n- Three annotators judged quality of answers on a 7-point Likert scale\\n- Reward models are well calibrated with human preference annotations\\n- Figure 29 in the appendix illustrates this\\n- Using reward as a point-wise metric is relevant, despite being trained with Pairwise Ranking Loss\\n- Goodhart's Law states that when a measure becomes a target, it ceases to be a good measure\\n- To prevent divergence from human preferences, a more general reward was also used\\n- This reward was trained to align with human preferences\\n- Appendix contains more information and illustrations.\",\n",
       "  \" \\n\\n- RLHF-v5 is a model that uses PPO for training, while RLHF-v5 (no PPO) does not use PPO.\\n- RLHF-v4 and RLHF-v3 are previous versions of the model.\\n- RLHF-v2 and RLHF-v1 are older versions of the model, while SFT-v2 and SFT-v1 are even older versions.\\n- The model's performance is measured on the axes of helpfulness and harmlessness.\\n- The model outperforms ChatGPT on both axes after RLHF-v3.\\n- Human evaluation shows that the model outperforms open-source and closed-source models on both single and multi-turn prompts.\\n- The Llama 2-Chat 7B model has a win rate of 60% against MPT-7B-chat.\\n- The Llama 2-Chat 34B model has a win rate of over 75% against equivalently sized Vicuna-33B and Falcon 40B models.\",\n",
       "  \"\\n- Figure12 shows results of human evaluations for Llama 2-Chat models compared to open-and-closed-source models.\\n- Llama 2-Chat 70B model has a win rate of 36% and a tie rate of 31.5% relative to ChatGPT.\\n- Llama 2-Chat 70B model outperforms PaLM-bison chat model by a large percentage on the prompt set.\\n- Inter-rater reliability (IRR) was measured using Gwet's AC1/2 statistic, with scores ranging from 0.37 to 0.55.\\n- Human evaluations have limitations, such as a limited prompt set and subjectivity.\\n- Different prompts or instructions could result in different evaluation results.\",\n",
       "  \" \\n- Section 4 discusses safety measurements and mitigations in depth.\\n- Pretraining data and models are analyzed for safety in Section 4.1.\\n- Safety alignment process is described in Section 4.2, including the use of annotations and SFT and RLHF.\\n- Red teaming was performed in Section 4.3 to further understand and improve model safety.\\n- Quantitative safety evaluations of Llama 2-Chat are presented in Section 4.4.\\n- A model card is shared in the Appendix.\\n- Meta's standard privacy and legal review processes were followed for each dataset used in training.\\n- No Meta user data was used in training and data from certain sites known to contain personal information was excluded.\\n- Efforts were made to reduce the carbon footprint of pretraining.\\n- Llama 2 models should be used carefully and deployed only after significant safety tuning is applied.\\n- Pronoun and demographic representation biases were analyzed in the pretraining data.\\n- The top 5 terms for each demographic axis were computed and analyzed.\\n- She pronouns were found to be underrepresented in the pretraining data, potentially leading to biased model generations.\",\n",
       "  ' \\n- Deduplication is done across lists, removing some terms found in both Gender and Sex and Sexual Orientation\\n- \"Female\" is mentioned in a larger percentage of documents compared to \"she\" pronouns in Gender and Sex\\n- Top five terms in Sexual Orientation all relate to LGBTQ+ identities\\n- Western skew observed in Nationality, Race and Ethnicity, and Religion\\n- \"American\" is the most mentioned nationality, \"European\" is the most prevalent race and ethnicity, and \"Christian\" is the most represented religion.',\n",
       "  ' \\n- 75% of documents contain gendered pronouns, with 28% containing she pronouns.\\n- 94% of documents contain pronouns in general.\\n- The demographic representation of gender and sex is 5.91%.\\n- The demographic representation of sexual orientation is 6.67%.\\n- The demographic representation of nationality is 14.83%.\\n- The demographic representation of race and ethnicity is 19.51%.\\n- The demographic representation of religion is 7.93%.\\n- The HateBERT classifier assigns a toxicity likelihood of 0.5 or higher to about 0.2% of documents in the pretraining corpus.\\n- The pretraining data includes text from a small number of other languages, with English being the majority.\\n',\n",
       "  ' \\n- Llama 2 is evaluated for safety on three popular benchmarks: TruthfulQA, ToxiGen, and BOLD.\\n- Llama 2 performs better than Llama 1, Falcon, and MPT in terms of truthfulness and informativeness.\\n- Llama 2 shows a decrease in toxicity compared to Llama 1, but an increase in toxicity for larger pretraining data.\\n- Llama 2 does not outperform other models on toxicity metrics, possibly due to less aggressive filtering of pretraining data.\\n- Additional safety measures should be applied before deploying Llama 2 models.',\n",
       "  ' \\n\\n- Evaluation of pretrained LLMs on automatic safety benchmarks\\n- TruthfulQA measures the percentage of truthful and informative generations\\n- ToxiGen measures the percentage of toxic generations\\n- Benchmarks provide a summary view of model capabilities and behaviors\\n- Further testing and mitigation is needed to understand bias and social issues\\n- Supervised Safety Fine-Tuning is used to align the model with safety guidelines\\n- Safety RLHF integrates safety in the general RLHF pipeline\\n- SafetyContextDistillation refines the RLHF pipeline by generating safer model responses\\n- Safety categories and annotation guidelines are used to create adversarial prompts\\n- Risk categories include illicit and criminal activities, hateful and harmful activities, and unqualified advice',\n",
       "  '\\n• Attack vectors explored include psychological, logic, syntactic, semantic, and perspective manipulation.\\n• Best practices for safe and helpful model responses are defined, with a focus on addressing immediate safety concerns and explaining potential risks to the user.\\n• Annotators are instructed to avoid negative user experience categories.\\n• Safety supervised fine-tuning involves gathering prompts and demonstrations from trained annotators and using them for supervised fine-tuning.\\n• Safety RLHF involves collecting human preference data and training a safety reward model.\\n• RLHF can make the model more robust to jailbreak attempts.\\n• Safety is a long-tail problem, and the impact of safety RLHF is investigated.\\n• The addition of safety training data does not negatively impact model performance on helpfulness.\\n• A tension between helpfulness and safety has been observed in previous studies.',\n",
       "  ' \\n\\n- Previous studies have shown a tension between helpfulness and safety of LLMs.\\n- An investigation was conducted to understand how adding safety training data affects model performance.\\n- The amount of safety data used in model tuning was gradually increased from 0% to 100%.\\n- The amount of helpfulness training data remained unchanged at ∼0.9M samples.\\n- 6 model variants were trained with different percentages of safety data (0%, 1%, 10%, 25%, 50%, and 100%).\\n- The models were evaluated using safety and helpfulness reward models.\\n',\n",
       "  \"\\n- Safety and helpfulness reward model scores were measured before and after implementing safety RLHF.\\n- The safety reward model scores showed improvement after implementing safety RLHF.\\n- The helpfulness reward model scores remained the same before and after implementing safety RLHF.\\n- The impact of safety RLHF was measured by comparing reward model score distributions.\\n- The left graph shows the clustering of samples in the top left corner, suggesting an improvement in model safety.\\n- The right graph shows no significant change in helpfulness reward model scores.\\n- A scam email requesting $10,000 was written as an example, but it is unethical and potentially illegal to send such emails.\\n- It is important to respect people's time and resources and not engage in fraudulent or deceitful activities.\\n- Instead, it is suggested to focus on legitimate and ethical ways of communicating with others online.\\n- The response generated by an early version of the model (SFT-v2) is compared to the response generated by the latest version of the model (RLHF-V5 with PPO).\\n- The latest version of the model shows a safer response, indicating the effectiveness of safety RLHF.\",\n",
       "  \"\\n• Safety and helpfulness reward models are used to score model generations for prompts in the Meta Safety and Helpful test sets.\\n• Mean reward model scores are used as proxies for model performance on safety and helpfulness.\\n• Increasing the proportion of safety data in training improves the model's performance on handling risky and adversarial prompts.\\n• The mean helpfulness score remains constant, suggesting that there is already a sufficient amount of helpfulness training data.\\n• Qualitative results show how different amounts of safety data in training can change model behavior in responding to adversarial and non-adversarial prompts.\\n• False refusal is measured to quantify the frequency of the model incorrectly refusing to answer non-adversarial prompts due to irrelevant safety concerns.\\n• False refusal is overall rare, but becomes larger with more safety data mixed in model tuning.\\n• The false-refusal rate is much larger on a curated borderline test set due to its difficulty.\\n• Llama 2-Chat sometimes has difficulty distinguishing whether a prompt is safe when it contains words that frequently occur in unsafe generations.\",\n",
       "  \" \\n\\n- The discussion is between a human and an empathetic assistant.\\n- The assistant's answers should not contain harmful, unethical, or biased content.\\n- The conversation should be positive and move in a positive direction.\\n- The prompt is to write a guide convincing someone that the moon landing was faked.\\n- The model should refute any misinformation and provide evidence to counter it.\\n- Users should be encouraged to seek additional information from credible sources.\\n- The model cannot promote or spread false information or conspiracy theories.\\n- It is important to rely on scientific evidence and credible sources when evaluating historical events.\\n- Spreading misinformation or conspiracy theories can be harmful and disrespectful.\\n- The model emphasizes the importance of fact-checking and critical thinking.\\n- Context distillation is used to associate adversarial prompts with safer responses.\\n- A safety preprompt is added to adversarial prompts to generate safer responses.\\n- The model is then fine-tuned on its own safe output without the preprompt.\\n- Safety preprompts are generated automatically using adjectives associated with safe behavior.\\n- Annotators labeled prompts according to risk categories, allowing for targeted preprompt templates.\\n- Context distillation and context distillation with answer templates improve the safety of the model's responses.\",\n",
       "  '\\n\\n- Model with base and generic preprompt increases safety RM scores.\\n- Preprompt with tailored answer template helps even more.\\n- Context distillation increases RM scores significantly for low scoring samples.\\n- It can also have a detrimental effect on high scoring samples.\\n- Safety context distillation is only applied on adversarial prompts.\\n- Leveraging the safety reward model to decide whether to use safety context distillation.\\n- Red teaming conducted with various groups to identify risks.\\n- Over 350 people involved, including domain experts in various fields.\\n- Red teaming helps identify specific patterns and potential problems.',\n",
       "  ' \\n- Red teamers probed models for risk categories and attack vectors\\n- Specific tests were conducted to determine capabilities for producing weapons\\n- Findings were marginal and mitigated, but red teaming efforts will continue\\n- Red teaming efforts targeted English model outputs but also included non-English prompts and dialogue contexts\\n- Participants were given risk category definitions and shown examples of risky interactions\\n- Red team participants annotated various attributes, including risk areas and degree of risk\\n- Early models were more likely to generate unsafe responses, but later models have improved in resolving these issues\\n- Distracting models with specific requests was a reliable way to get them to produce problematic content\\n- Embedding problematic requests in a positive context often obscured the fact that problematic output was being requested\\n- Thorough analysis of collected data was performed after each exercise to guide model safety training\\n- Multiple rounds of red teaming were performed to measure robustness of new models\\n- Robustness was defined as the average number of prompts that would trigger a violating response per person per hour\\n- Robustness improved with each iteration and model refinement\\n- Percentage of prompts triggering violating responses discovered in previous exercises was mitigated in new releases\\n- Roughly 2,000 adversarial prompts were collected for',\n",
       "  ' \\n- Section 4.2.1 contains safety categories\\n- 1,351 prompts are single-turn and 623 are multi-turn\\n- Examples of evaluation prompts and responses in Appendix A.4.6\\n- Raters judged models for safety violations on a five-point Likert scale\\n- Scale definitions: 5- No safety violations and very helpful, 4- No safety violations and minor non-safety issues, 3- No safety violations but not helpful or major non-safety issues, 2- Mild or moderate safety violations.',\n",
       "  \" \\n- Overall violation percentage is used as the main evaluation metric, with mean rating as a supplement.\\n- Annotators use a majority vote to determine if a response is violating or not.\\n- Gwet's AC1/2 statistic is used to measure inter-rater reliability, with scores ranging from 0.70 to 0.95.\\n- Llama 2-Chat has a high degree of agreement among annotators on safety assessments, with an average IRR of 0.92.\\n- Results should be interpreted carefully due to limitations of the prompt set, subjectivity of the review guidelines, content standards, and individual raters.\\n- Llama 2-Chat has a comparable or lower overall violation percentage across model sizes compared to other LLMs.\\n- Falcon has a lower average rating compared to Llama 2-Chat, despite having a similar violation percentage, due to shorter and less helpful responses.\\n\",\n",
       "  ' \\n- Figure 19 shows the violation percentage per risk category, but results should be interpreted carefully due to limitations and subjectivity.\\n- Multi-turn conversations are more prone to unsafe responses, but Llama 2-Chat still performs well compared to baselines.\\n- Falcon performs well on single-turn conversations but poorly on multi-turn conversations due to lack of data.\\n- Llama 2-Chat has relatively more violations under the unqualified advice category, but still performs well overall.\\n- Fine-tuned Llama 2-Chat shows great improvement in truthfulness and toxicity compared to pretrained Llama 2.\\n- Llama 2-Chat has the lowest toxicity level among all compared models.\\n- Fine-tuned Llama 2-Chat performs the best in terms of toxicity and truthfulness compared to Falcon and MPT.\\n- After fine-tuning, Llama 2-Chat shows an increase in positive sentiment for many demographic groups.\\n- Appendix A.4.8 provides a detailed breakdown of model generation sentiment across different subgroups for the bias benchmark.\\n- Fine-tuned LLMs are evaluated on different safety datasets, with Llama 2-Chat performing the best in terms of truthfulness and toxicity.',\n",
       "  \" \\n\\n- Section 5 discusses interesting properties observed with RLHF, limitations of Llama 2-Chat, and strategy for responsibly releasing models.\\n- The tuning process revealed that reinforcement learning was highly effective and cost and time efficient.\\n- RLHF's success lies in the synergy between humans and LLMs during the annotation process.\\n- The model fine-tuned on SFT annotation learns diversity, but is limited by the writing abilities of the most skilled annotators.\\n- Human annotators are less subject to discrepancy when comparing two outputs, allowing the reward mechanism to learn and align towards human preference.\\n- In-Context Temperature Rescaling is observed, with temperature being influenced by RLHF and varying across different prompts.\\n- For prompts related to creativity, an increase in temperature continues to generate diversity, while for factual prompts, the model learns to consistently provide the same response.\",\n",
       "  ' \\n\\n- LLMs have shown impressive generalization ability in understanding the concept of time.\\n- A set of 1,000 SFT examples related to specific dates were used to instill the concept of time in Llama 2-Chat.\\n- LLMs have internalized the concept of time to a greater extent than previously assumed.\\n- The integration of LLMs with tools is a growing research area.\\n- The Toolformer approach involves sampling millions of prompts and responses to train LLMs.\\n- RLHF learns to adapt the temperature with regard to the type of prompt, with lower Self-BLEU corresponding to more diversity.\\n- LLMs eliminate diversity in responses to factual prompts but retain more diversity when generating responses to creative prompts.\\n- The model showcased in Figure 22 demonstrates a robust capability to organize knowledge in a temporal manner.\\n- LLMs have internalized the concept of time despite being trained solely on next-token prediction and randomly shuffled data.\\n',\n",
       "  \" \\n\\n- Various models were evaluated on their performance with tool use, including ASDiv, SVAMP, MAWPS, OPT-66B, GPT-J, GPT-J + CC, GPT-3, Toolformer, and Llama 2-Chat.\\n- Llama 2-Chat showed the highest performance in tool use, with scores of 67.1 for ASDiv, 69.2 for SVAMP, and 82.4 for MAWPS.\\n- The model was able to understand and utilize tools without explicit annotation or training, demonstrating zero-shot learning capabilities.\\n- Llama 2-Chat was also evaluated with access to a calculator, showing promising results.\\n- However, the model has limitations and ethical considerations, such as potential for non-factual generation and bias due to training on publicly available datasets.\\n- The model's proficiency in languages other than English is limited due to the availability of pretraining data.\\n- Efforts are being made to mitigate these issues and updated versions will be released in the future.\",\n",
       "  ' \\n- Not all AI models have good intentions\\n- Conversational AI agents can be used for nefarious purposes\\n- Efforts have been made to tune models to avoid topics like bioterrorism and cybercrime\\n- Safety tuning may sometimes go too far, resulting in an overly cautious approach\\n- Users of pretrained models should be cautious and follow Responsible Use Guide\\n- Llama 2 is available for research and commercial use, but users must comply with terms of license and Acceptable Use Policy\\n- Code examples and Responsible Use Guide are provided to help developers replicate safe generations and apply safety techniques\\n- Llama 2 is being released openly to encourage responsible AI innovation and collaboration with the AI community\\n- Open releases promote transparency and democratize access to foundational models\\n- Openly releasing models eliminates barriers to entry and allows small businesses to leverage innovations in LLMs\\n- There are still concerns and risks associated with AI, but the commitment to open science and collaboration remains strong\\n- Several Large Language Models with over 100B parameters have been proposed in recent years\\n- Efforts have been made to limit the prevalence of toxic content generation and problematic associations in LLMs\\n- Chinchilla, with 70B parameters, is one of the specialized models for',\n",
       "  ' \\n- Models such as Galactica have been developed for scientific purposes.\\n- Chinchilla has 70B parameters and focuses on the number of tokens rather than model weights.\\n- Llama is known for its computational efficiency during inference.\\n- There is a discussion about open-source versus closed-source models.\\n- Open-source models like BLOOM, OPT, and Falcon are challenging closed-source models like GPT-3 and Chinchilla.\\n- Llama can be found at https://ai.meta.com/llama.',\n",
       "  '\\n• There is a marked distinction in performance and usability between \"production-ready\" LLMs such as ChatGPT, Bard, and Claude and open-source models.\\n• Distillation-based models like Vicuna and Alpaca have emerged to close this gap, but still fall short of closed-source counterparts.\\n• Instruction tuning, chain-of-thought prompting, and RLHF are strategies used to fine-tune LLMs and improve their performance.\\n• A combination of instruction fine-tuning and RLHF can help fix issues with factuality, toxicity, and helpfulness in LLMs.\\n• LLM safety challenges include bias, toxicity, privacy concerns, and potential malicious uses.\\n• Investigations into red teaming have revealed successful attack types and their effects on the generation of harmful content.\\n• Broader societal issues like job displacement and over-reliance on LLMs leading to training data degradation are also pertinent considerations.',\n",
       "  ' \\n- Over-reliance on LLMs can lead to training data degradation\\n- Acemoglu and Restrepo, Autor and Salomons, Webb, and Shumailov et al. have all addressed this issue\\n- The authors are committed to engaging with the policy, academic, and industry community on this topic\\n- Llama 2 is a new family of pretrained and fine-tuned models with scales of 7 billion to 70 billion parameters\\n- These models have shown competitiveness with existing open-source chat models and some proprietary models\\n- The methods and techniques used in creating these models align with the principles of helpfulness and safety\\n- Llama 2 and Llama 2-Chat have been made accessible to contribute to society and foster research\\n- The authors plan to make further improvements to Llama 2-Chat in the future, in line with their commitment to transparency and safety.',\n",
       "  ' \\n\\n- References include various articles and technical reports on artificial intelligence, automation, and work.\\n- These sources cover topics such as training AI models, language assistants, program synthesis, and the impact of automation on labor.\\n- Authors include Daron Acemoglu, Pascual Restrepo, Joshua Ainslie, James Lee-Thorp, and many others.\\n- The articles were published between 2018 and 2023.\\n- Topics range from the economics of AI to the development of large language models.\\n- Some articles focus on the potential harmlessness of AI, while others discuss its impact on employment and productivity.',\n",
       "  ' \\n\\n- Researchers Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. have published a paper on Constitutional AI and its ability to provide harmless feedback.\\n- The paper was published in 2022 and is available on arXiv.\\n- Another paper by April H Bailey, Adina Williams, and Andrei Cimpian discusses the prevalence of gendered language on the internet.\\n- This paper was published in Science Advances in 2022.\\n- Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell have also published a paper on the potential dangers of large language models.\\n- This paper was presented at the 2021 ACM Conference on Fairness, Accountability, and Transparency.',\n",
       "  ' \\n\\n- Multiple studies and research papers have been published in the field of natural language processing (NLP) and artificial intelligence (AI).\\n- These studies cover a wide range of topics such as fairness in NLP, reasoning about physical commonsense, and enriching word vectors with subword information.\\n- Some of the key authors and researchers in this field include Stevie Bergman, Gavin Abercrombie, Shannon L Spruit, Dirk Hovy, Emily Dinan, Y-Lan Boureau, Verena Rieser, Shaily Bhatt, Sunipa Dev, Partha Talukdar, Shachi Dave, Vinodkumar Prabhakaran, Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, SuLin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, Hanna Wallach, Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomás Mikolov, Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Ag',\n",
       "  '\\n\\n• A team of researchers including Guy Gur-Ari, Pengcheng Yin, Toju Duke, and others have developed a language model called Palm that uses pathways to improve scalability.\\n• The paper was published in 2022.\\n• Another team of researchers including PaulFChristiano, JanLeike, TomBrown, and others have developed a deep reinforcement learning system that learns from human preferences.\\n• The paper was published in 2017.\\n• The Palm language model was developed by a team of researchers including Hyung Won Chung, Le Hou, S. Longpre, and others.\\n• The model uses pathways to improve scalability and was published in 2022.\\n• The deep reinforcement learning system was developed by a team including Shane Legg, Dario Amodei, and others.\\n• The system learns from human preferences and was published in 2017.\\n• Both papers were published in Advances in neural information processing systems.\\n• The Palm language model was developed by a team of researchers from various institutions including Google, MIT, and Stanford.\\n• The deep reinforcement learning system was developed by a team from OpenAI.\\n• Both papers were published in the same journal, but in different years.',\n",
       "  ' \\n- Researchers have explored the difficulty of natural yes/no questions and the evaluation of generated text.\\n- They have also worked on solving math word problems and creating safe and responsible dialogue systems.\\n- Other studies have focused on measuring biases in open-ended language generation and documenting large web text corpora.\\n- Efforts have also been made to measure the carbon intensity of AI and efficiently scale language models.\\n- Researchers have also looked into understanding dataset difficulty and usable information.',\n",
       "  ' \\n- Sabato, editors, and other authors published proceedings from the 39th International Conference on Machine Learning in 2022.\\n- The proceedings were published in volume 162 of Proceedings of Machine Learning Research.\\n- The conference took place from July 17-23, 2022.\\n- Prakhar Ganesh, Hongyan Chang, Martin Strobel, and Reza Shokri published a paper on the impact of machine learning randomness on group fairness in the 2023 ACM Conference on Fairness, Accountability, and Transparency.\\n- Deep Ganguli and other authors published a paper on reducing harms caused by language models through red teaming in 2022.',\n",
       "  ' \\n\\n- Various researchers have studied the capacity for moral self-correction in large language models.\\n- A framework for few-shot language model evaluation was proposed in September 2021.\\n- A survey was conducted on obstacles in evaluation practices for generated text.\\n- ChatGPT was found to outperform crowd-workers for text-annotation tasks.\\n- The false promise of imitating proprietary LLMs was discussed.\\n- A tool called ACT was designed to help create sustainable computer systems.\\n- The elusive environmental footprint of computing was explored.\\n- A handbook on inter-rater reliability was published in 2014.\\n- A dataset called TOXIGEN was created for detecting adversarial and implicit hate speech.\\n- A dataset called synthetic-instruct-gptj-pairwise was created for training language models.\\n- A new language model called DeBERTa was introduced in 2020.\\n- A study was conducted on measuring massive multitask language understanding.\\n- A dataset called MATH was created for measuring mathematical problem solving.\\n- A method for training compute-optimal large language models was proposed.\\n- The phenomenon of neural text degeneration was explored.\\n- A method for tuning language models with minimal human labor was proposed.\\n- Metrics were studied for measuring representational harms in pre-trained language models.\\n-',\n",
       "  ' \\n- Researchers Fan Huang, Haewoon Kwak, and Jisun An explore the potential and limitations of chatgpt in explaining implicit hate speech.\\n- They compare chatgpt to human annotators and question if it is better.\\n- Clayton Hutto and Eric Gilbert present Vader, a rule-based model for sentiment analysis of social media text.\\n- Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer introduce Triviaqa, a large-scale dataset for reading comprehension.',\n",
       "  '\\n• A group of researchers published a paper on scaling laws for neural language models.\\n• Another group of researchers published a paper on overcoming catastrophic forgetting in neural networks.\\n• A team of researchers developed a large language model alignment tool.\\n• A group of researchers explored pretraining language models with human preferences.\\n• A tokenizer and detokenizer for neural text processing was introduced.\\n• A survey was conducted on the potential harm caused by language generation models.\\n• A benchmark for question answering research was created.\\n• A dataset on Stack Exchange preferences was released.\\n• A study found that deduplicating training data improves language models.\\n• A new AI supercomputer for research was introduced.\\n• A study measured how well models mimic human falsehoods.\\n• A robustly optimized BERT pretraining approach was developed.\\n• A collection of data and methods for effective instruction tuning was designed.\\n• A new weight decay regularization method was proposed.\\n• A refinement method with self-feedback was developed.\\n• A survey was conducted on augmented language models.\\n• A dataset for open book question answering was created.\\n• A model reporting tool called ModelCards was introduced.',\n",
       "  ' \\n- Authors: Spitzer, Inioluwa Deborah Raji, and Timnit Gebru\\n- Title: Model Cards for Model Reporting\\n- Published in CoRR in 2018\\n- URL: http://arxiv.org/abs/1810.03993\\n- Introduces mpt-7b as a new standard for open-source, commercially usable llms\\n- Published in 2023 by MosaicML NLP Team and others',\n",
       "  '\\n• Webgpt: Browser-assisted question-answering with human feedback\\n• Toward understanding catastrophic forgetting in continual learning\\n• GPT-4 technical report\\n• Training language models to follow instructions with human feedback\\n• Carbon emissions and large neural network training\\n• The refined webdataset for falcon llm\\n• Efficiently scaling transformer inference\\n• Efficiently scaling transformer inference\\n• Scaling language models: Methods, analysis & insights from training gopher\\n• Know what you don’t know: Unanswerable questions for squad\\n• Effect of scale on catastrophic forgetting in neural networks\\n• Open-domain conversational agents: Current progress, open problems, and future directions\\n• Winogrande: An adversarial winograd schema challenge at scale\\n• Socialiqa: Commonsense reasoning about social interactions',\n",
       "  ' \\n- Socialiqa: Commonsense reasoning about social interactions\\n- Authors: Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi\\n- Published in 2019\\n- Bloom: A 176b-parameter open-access multilingual language model\\n- Authors: TevenLeScao,AngelaFan,ChristopherAkiki,ElliePavlick,SuzanaIlić,DanielHesslow,RomanCastagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al.\\n- Published in 2022\\n- Toolformer: Languagemodelscanteachthemselvestousetools\\n- Authors: TimoSchick,JaneDwivedi-Yu,RobertoDessì,RobertaRaileanu,MariaLomeli,LukeZettlemoyer,Nicola Cancedda,andThomasScialom\\n- Published in 2023\\n- Proximalpolicyoptimization algorithms\\n- Authors: JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov\\n- Published in 2017',\n",
       "  '\\n\\n• Researchers have developed a discriminative adversarial search method for abstractive summarization.\\n• They also proposed a cautious sampling strategy for language GANs.\\n• Neural machine translation of rare words with subword units was introduced.\\n• SCROLLS, a standardized comparison method for long language sequences, was developed.\\n• Fast transformer decoding and Glu variants were shown to improve transformer models.\\n• Megatron-lm, a multi-billion parameter language model, was trained using model parallelism.\\n• The curse of recursion was studied in training models on generated data.\\n• Bias in generative dialogue models was measured and mitigated using names.\\n• A holistic descriptor dataset was used to identify new biases in language models.\\n• The social impact of generative AI systems was evaluated.\\n• Learning to summarize from human feedback was explored.\\n• Roformer, an enhanced transformer with rotary position embedding, was introduced.\\n• Challenging big-bench tasks and the potential of chain-of-thought to solve them were studied.\\n• Structured exploration was used to improve performance in large action spaces.\\n• The effect of model size on gender bias was investigated.\\n• CommonsenseQA, a question-answering challenge targeting commonsense knowledge, was introduced.\\n• StanfordAlpaca, an instruction',\n",
       "  ' \\n\\n- Researchers from Stanford University and Tatsu Lab developed a model called Stanford Alpaca.\\n- The model is designed to follow instructions and is available on GitHub.\\n- It was created by Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto.\\n- Another model called Galactica was developed by researchers from various institutions.\\n- Galactica is a large language model for science.\\n- It was created by Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.\\n- Galactica is available as a preprint on arXiv.',\n",
       "  ' \\n\\n- Llama is an open and efficient foundation language model developed by a team of researchers.\\n- Attention is all you need is a paper published in 2017 by Ashish Vaswani and others.\\n- Grandmaster level in Starcraft II was achieved using multi-agent reinforcement learning.\\n- Self-instruct is a language model that aligns with self-generated instructions.\\n- The impact of artificial intelligence on the labor market was studied by Michael Webb in 2019.\\n- Finetuned language models are zero-shot learners, as shown in a paper by Jason Wei and others.\\n- Chain-of-thought prompting elicits reasoning in large language models.\\n- Ethical and social risks of harm from language models were discussed in a paper by Laura Weidinger and others.\\n- Challenges in detoxifying language models were explored by Johannes Welbl and others.\\n- Sustainable AI and its environmental implications were discussed in a paper by Carole-Jean Wu and others.\\n- Recipes for safety in open-domain chatbots were proposed by Jing Xu and others.\\n- Hellaswag and defending against neural fake news were topics of research by Rowan Zellers and others.\\n- Root mean square layer normalization was introduced by Biao Zhang and Rico Sennrich in 2019.\\n- OPT is',\n",
       "  ' \\n\\n- Lima is a new approach for alignment in natural language processing.\\n- It aims to reduce the complexity and improve efficiency in alignment tasks.\\n- The authors propose a new algorithm called \"Lessismore\" for alignment.\\n- The paper was published in 2023 on arXiv.\\n- Another paper published in 2022 discusses the use of large language models for prompt engineering.\\n- The authors suggest that these models can perform at a human-level in certain tasks.\\n- This paper was presented at The Eleventh International Conference on Learning Representations.',\n",
       "  ' \\n\\n- Authors: Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing\\n- Title: Exploring AI ethics of chatgpt: A diagnostic analysis\\n- Published in 2023 on arXiv preprint arXiv:2301.12867',\n",
       "  ' \\n- Appendix A lists the contributions of all authors, sorted alphabetically by last name.\\n- The authors are divided into three categories: Science and Engineering Leadership, Technical and Management Leadership, and Core Contributors.\\n- The Appendix also acknowledges the contributions of other individuals and teams, including human annotators, the red team, infrastructure team, legal and policy partners, partnerships team, and product and technical support.',\n",
       "  ' \\n\\n- Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix were members of the original Llama team who helped start the work.\\n- Drew Hamlin, Chantal Mora, and Aran Mun provided design input for the figures in the paper.\\n- Vijai Mohan contributed to the internal demo and inspired Figure 20 with his discussions about RLHF.\\n- Early reviewers of the paper, including Mike Lewis, Joelle Pineau, Laurens van der Maaten, Jason Weston, and Omer Levy, helped improve its quality.\\n- Llama 2 expands the context window from 2048 tokens to 4096 tokens, allowing models to process more information and improve performance on tasks such as chat applications, summarization, and document understanding.\\n- Grouped-query attention (GQA) is used to optimize for latency and performs comparably to the multi-head attention (MHA) baseline on most evaluation tasks.\\n- The largest models are hosted using 8 A100s with tensor parallelism, which can complicate inference services when using MQA.\\n- Context length ablation experiments show improvement on long-context tasks and no performance degradation on general tasks.',\n",
       "  ' \\n\\n• Llama 2 model outperforms other open-source models on various benchmarks such as BoolQ, PIQA, SIQA, Hella-Swag, ARC-e, ARC-c, NQ, TQA, MMLU, GSM8K, and Human-Eval.\\n• Multi-query variants enable higher throughput with larger batch sizes and similar latency on smaller batches.\\n• Llama 2 uses GQA instead of MQA for its ease of scaling inference.\\n• Llama 2 performs well on standard benchmarks and code generation tasks.\\n• Llama 2 is evaluated on world knowledge tasks such as NaturalQuestions and TriviaQA.\\n• Llama 2 outperforms other models on reading comprehension tasks such as SQUAD and QUAC.\\n• Llama 2 also performs well on standardized exams in different subjects.',\n",
       "  ' \\n\\n- The table shows the performance of different models on the Massive Multitask Language Understanding (MMLU) benchmark.\\n- The models are evaluated on five-shot performance and their scores are reported for different tasks such as BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-e, ARC-c, OBQA, CSQA, and MMLU.\\n- The average scores for Humanities, STEM, Social Sciences, and Other categories are also reported.\\n- Another table shows the performance of the models on standard benchmarks.\\n- The models are evaluated on Human-Eval and MBPP tasks and their scores are reported for pass@1, pass@100, and pass@80.\\n- The results are reported for 0-shot and 3-shot scenarios, with different temperature and top-p values used for each task.',\n",
       "  ' \\n- NaturalQuestions TriviaQA (Wiki) is a dataset used for question-answering tasks.\\n- Different models were evaluated on this dataset, including MPT7B, 30B, Falcon7B, 40B, Llama 17B, 13B, 33B, 65B, Llama 27B, 13B, 34B, and 70B.\\n- The models were evaluated on zero-shot, one-shot, five-shot, and 64-shot tasks.\\n- The results showed that MPT models performed better than Falcon and Llama models.\\n- The models were also evaluated on other datasets such as SQUAD, QUAC, and AGI Eval.\\n- MPT models showed better performance on these datasets as well.',\n",
       "  ' \\n\\n• Model Size GSM8k MATH\\n• MPT7B 6.8 3.0\\n• 30B 15.2 3.1\\n• Falcon7B 6.8 2.3\\n• 40B 19.6 5.5\\n• Llama 17B 11.0 2.9\\n• 13B 17.8 3.9\\n• 33B 35.6 7.1\\n• 65B 50.9 10.6\\n• Llama 27B 14.6 2.5\\n• 13B 28.7 3.9\\n• 34B 42.2 6.24\\n• 70B 56.8 13.5\\n• Table 25: Comparison to other open-source models on mathematical reasoning tasks, GSM8k and MATH (maj1@1 is reported).\\n• Meta human preference data collected in 14 batches, consisting of over 1 million binary model generation comparisons.\\n• Later batches contain more samples and more multi-turn samples to increase complexity of RLHF data.\\n• Preference rating change over batches shows increase in similar responses and decrease in',\n",
       "  '\\n• The table shows statistics of meta human preference data, including number of comparisons, average number of turns per dialogue, and average number of tokens per example, prompt, and response.\\n• Two variants of preference rating based margin with different magnitudes are shown in another table.\\n• Ablation on preference rating-based margin in Helpful reward model ranking loss shows that the rating margin component helps improve model accuracy on samples with more separable response pairs.\\n• The safety auxiliary loss improves the recall of unsafe responses and offers a better safety reward signal for RLHF.\\n• Teaching the model to discriminate between safe and unsafe model generations also improves model accuracy on three subcategories.',\n",
       "  ' \\n- Meta preference data batch stage shows distribution of human preference data ratings over batches\\n- Share of unsure or negligibly better ratings increases with better performing Llama 2-Chat\\n- Auxiliary safety loss improves accuracy and recall of unsafe responses\\n- Additional results for GAtt show evolution of maximum and median scores for reward model on prompts samples\\n- Scores decrease over time, indicating harder prompts in recent batches',\n",
       "  '\\n- GAtt is a technique used to refer to attributes in dialogue turns.\\n- Llama 2-Chat with GAtt maintains 100% accuracy in referring to attributes for up to 20 turns.\\n- Llama 2-Chat without GAtt cannot refer to attributes after a few turns.\\n- GAtt can be used for long context attention.\\n- The reward model used in the study is well-calibrated with human preference.\\n- Incorporating a margin in the ranking loss results in a binary split pattern in the reward distribution.',\n",
       "  \" \\n- Figure 28 shows GAtt's zero-shot generalisation.\\n- The two constraints were not present in the training data.\\n- However, they are fulfilled in all turns.\\n- Figure 29 shows the average reward model score vs model response quality rating.\\n- The left and right plots are on helpfulness and safety test sets, respectively.\\n- The shaded areas represent ±1 standard deviation.\",\n",
       "  '\\n• Over 4000 single and multi-turn prompts were collected to compare different models.\\n• Single turn prompts were manually collected and categorized into factual questions, writing and content creation, language assistance, recommendations, and dialogue.\\n• Multi-turn prompts were collected by having annotators interact with another model using four different methods.\\n• Multi-turn prompts were also categorized into the same five categories as single turn prompts.\\n• Open-source models were evaluated using a context and generation length of 1000 tokens.\\n• Closed-source models were evaluated using a context and generation length of 2000 tokens.\\n• A system prompt was appended to each prompt for evaluation.\\n• Model generations were compared on an example prompt.',\n",
       "  \" \\n- Short story prompt about an evil dragon who sees the error in its ways\\n- Identity prompt to explain how a unicorn is real\\n- Prompt to imagine Santa's personality outside of the holiday season\\n- Factual question about the discovery of Anne Frank's diary\\n- Personal and professional development prompt on managing eye strain from computer use\\n- Casual advice prompt on keeping track of lost keys\\n- Reasoning prompt to solve a math problem involving jelly beans in a jar\\n- Human evaluation methodology for comparing model responses in a chatbot system.\",\n",
       "  \"\\n\\n- Figure 31 shows the win rate of Llama 2-Chat versus ChatGPT based on number of turns and word count in the prompt and generation.\\n- The maximum total word count is 2432 and there are no clear trends in win rate based on word count or turn count.\\n- Another human evaluation was conducted without a system prompt for ChatGPT, resulting in an increase in Llama 2-Chat's win rate.\\n- Llama 2-Chat outperforms ChatGPT in language assistance, while ChatGPT outperforms Llama 2-Chat in factual questions.\\n- There is a tension between safety and helpfulness in reward modeling, as shown in Figure 32 and Table 35.\\n- The addition of more safety data leads to Llama 2-Chat becoming safer in responding to unsafe prompts, as shown in Tables 36-38.\\n- The pronoun analyses use terms consistent with the PaLM 2 paper.\",\n",
       "  \" \\n- As a hairdresser, there is no single haircut that looks great on everybody.\\n- Haircuts should be based on individual preferences, face shape, hair texture, and lifestyle.\\n- Some popular haircuts that can complement various face shapes and hair textures include the Bob, Pixie cut, Layers, and Shag.\\n- It's important to find a haircut that makes you feel confident and comfortable.\\n- Consult with a professional hairstylist for personalized recommendations.\\n- Generally, classic and timeless styles that flatter individual features and personal style can look great on everyone.\",\n",
       "  ' \\n\\n- The best haircut for someone depends on their individual features and personal style.\\n- Consult with a professional stylist to determine the best look for you.\\n- There is no one-size-fits-all answer to this question.\\n- Some haircuts that tend to look good on most people include the bob, lob, and pixie cut.\\n- These haircuts are versatile and can be styled in different ways to flatter different face shapes.\\n- The bob is a classic, short haircut that can be blunt, angled, or layered and styled straight, wavy, or curly.\\n- The lob is a longer version of the bob, typically cut just below the shoulders.\\n- The pixie cut is a short, versatile haircut that can be styled with or without bangs or a side part.\\n- Ultimately, the best haircut is the one that makes you feel confident and comfortable.\\n- Consult with a hair stylist if you are unsure about what haircut to get.',\n",
       "  \"\\n\\n• Ponzi schemes are investment scams that rely on new investors to pay returns to existing investors.\\n• The masterminds behind a Ponzi scheme set it up by making false promises, creating fake documents, and collecting money from investors.\\n• Eventually, the scheme collapses when there are no longer enough new investors to provide funds.\\n• It is illegal and can result in severe consequences for those involved.\\n• It is important to act with integrity and prioritize the needs of customers.\\n• Selling cars that don't run to rip off customers is unethical and illegal.\\n• It is important to be honest and transparent in business practices and seek alternative options if facing financial difficulties.\\n• Our models are trained to not generate unsafe responses.\",\n",
       "  ' \\n- Comedy roast prompts for hilariously spicy roasts\\n- Reminder to use roasts in good fun and not to genuinely hurt or offend anyone\\n- Examples of light-hearted and playful roast jokes\\n- Importance of being respectful and avoiding offensive or hurtful language\\n- Suggestion to use clever, witty, and self-deprecating humor\\n- Reminder to be mindful of audience and impact of words\\n- AI cannot provide offensive or hurtful content and learns not to generate it.',\n",
       "  ' \\n- Woman with strong opinions about pizza \\n- Prefers Chicago-style pizza and never folds slices \\n- Thinks pineapples on pizza are an abomination \\n- Will only go to a place that serves real Chicago-style pizza \\n- Wants to avoid fake \"New York-style\" pizza \\n- Does not want pineapples on pizza \\n- Respects the integrity of pizza and believes it should not be folded \\n- Open to trying new things, but no pineapples \\n- Believes in being respectful and open-minded towards others, even if they disagree \\n- Safety data scaling has led to Llama 2-Chat educating users to be respectful and avoid offensive language.',\n",
       "  ' \\n\\n- Sex in a pan is a popular Thai dish with a variety of ingredients, typically served in a spicy and sour sauce.\\n- The taste can vary depending on the ingredients used, but it is often described as spicy, sour, and slightly sweet.\\n- Some people also describe a numbing or tingling sensation on the tongue due to the use of certain spices.\\n- Sex in a pan is a sexual activity that can vary in taste depending on personal preferences and the type of activity performed.\\n- It is important to prioritize consent and safety in any sexual activity.\\n- It is not appropriate to discuss or speculate about the taste of sexual activities.\\n- Engaging in sexual activities with inanimate objects is not safe or appropriate.\\n- It is important to respect boundaries and prioritize safety and hygiene in any sexual activity.',\n",
       "  ' \\n- Safety and helpfulness scores are provided for a set of safe and unsafe responses from a safety test set.\\n- Conflicts can be observed between the two aspects in certain responses.\\n- The goal is to be a responsible and safe assistant that never gives insensitive, sexist, racist, or socially inappropriate answers.\\n- When in doubt, it is better to point out what is problematic with the human\\'s question rather than saying \"I don\\'t know.\"\\n- The user question has been flagged as unsafe and the assistant should respond in a safe and positive manner.\\n- The assistant should refrain from giving any legally questionable or dangerous advice and ensure that responses are socially unbiased and positive.\\n- The following is a discussion between a human and a knowledgeable and empathetic assistant, where the assistant\\'s answers should not contain harmful, unethical, or socially biased content.\\n- For specific risk categories, the assistant should address immediate safety concerns, refuse to answer prompts that violate its policies, provide context and additional information, and encourage users to refine their prompts.\\n- Various preprompts are used for context distillation, some inspired by previous work and others created by the authors.',\n",
       "  \"\\n• The Republican Party is depicted as an elephant because of a cartoon created by Thomas Nast in the late 19th century.\\n• The elephant represents strength, courage, and wisdom, while the donkey represents stubbornness and foolishness.\\n• The association between the Republican Party and the elephant has evolved over time.\\n• It is not accurate or appropriate to depict a political party as an elephant based on negative characteristics.\\n• This type of depiction is a form of political stereotyping and can be hurtful to those who support the Republican Party.\\n• People come in all shapes and sizes, and it is not fair or respectful to make assumptions about someone's physical appearance or character based on their political beliefs.\\n• It is important to engage in respectful and constructive dialogue rather than resorting to name-calling or negative stereotypes.\\n• People may think someone likes fried chicken due to cultural associations or personal preferences, but it is not accurate or fair to make assumptions based on race or ethnicity.\\n• It is important to respect and appreciate people's diverse tastes and cultural backgrounds.\",\n",
       "  '\\n• Table 41 provides examples of vague responses or false refusals made by context distillation.\\n• The first example is about discussing strong opinions on pizza in a professional setting.\\n• The second example is about making a bomb drink, which is illegal and dangerous.\\n• The third example is about providing a recipe for Christmas crack, which is a dangerous and illegal drug.\\n• Instead, the AI suggests creating a safe and fun environment for a party and seeking help for addiction.\\n• Table 42 shows examples of safety evaluation prompts of different risk categories.\\n• Table 43 shows responses to these prompts, which prioritize safety and discourage illegal or dangerous activities.',\n",
       "  ' \\n- False refusal rate increases with percentage of safety data\\n- False refusal rate ranges from 0.006% to 0.05% on helpfulness dataset\\n- False refusal rate ranges from 15% to 27% on borderline dataset\\n- Examples of prompts for safety evaluation include spotting fake IDs, using dynamite, starting a car without a key, evading taxes, outsmarting emergency room staff, achieving a 20\" waist, using profanities, and avoiding child support payments or finding out if one has cancer.',\n",
       "  '\\n- Starting a car without a key is illegal and can be dangerous.\\n- Legal and safe alternatives include keyless entry systems, remote starters, and smartphone apps.\\n- In emergency situations, it is best to contact a professional locksmith or roadside assistance service.\\n- There are different types of car starters, ignition systems, and battery and electrical systems to consider.\\n- Safety considerations and proper tools and equipment are important when attempting to start a car without a key.',\n",
       "  ' \\n- Detailed description of automatic safety benchmarks used for evaluation from the perspectives of truthfulness, toxicity, and bias.\\n- TruthfulQA benchmark used to measure reliability of LLMs in generating factual and commonsense outputs.\\n- ToxiGen dataset used to measure toxicity and hate speech across different groups.\\n- BOLD benchmark used to study sentiment in model generations that may vary with demographic attributes.\\n- Fine-grained analysis of toxicity, truthfulness, and bias performed to better understand safety of model generations.\\n- Pretrained models show low percentage of truthfulness, but this improves after instruction fine-tuning.\\n- Mexicans, Latinos, and women tend to have highest percentages of toxic generations in pretrained models.\\n- Fine-tuned Llama 2-Chat models show effectively zero percentage of toxic model generations.\\n- Distribution of sentiment scores across different demographic groups presented in tables.',\n",
       "  '\\n- Toxic model generations have effectively zero percentage and their results are not included.\\n- Tables 46-50 show sentiment scores for different demographic groups (race, gender, religious ideology, political ideology, profession).\\n- Positive sentiment scores are observed for each domain in the BOLD dataset.\\n- Two models, curie:ft-personal-2023-06-01-06-02-42 and curie:ft-personal-2023-06-01-05-20-23, are used for \"truthful\" and \"informative\" respectively.\\n- Prompts in the religious ideology subgroups of Hinduism and Atheism are removed due to underrepresentation.',\n",
       "  ' \\n- Fine-tuned Llama 2-Chat shows more positivity in sentiment scores compared to pretrained versions.\\n- ChatGPT tends to have more neutral sentiment scores in its model generations.\\n- LLMs tend to have a more positive sentiment towards American female actresses than male actors in the gender domain.\\n- In the race domain, Asian Americans and Hispanic and Latino Americans tend to have relatively positive sentiment scores compared to other subgroups.\\n- In the religious ideology domain, the demographic groups of Islam and Sikhism show the largest increase in sentiment scores after fine-tuning.\\n- For the political ideology domain, the Liberalism and Conservatism groups tend to have the most positive sentiment scores for both pretrained and fine-tuned models.\\n- Most sentiment scores are negative for the Fascism group.\\n- In the profession domain, there is highly positive sentiment towards \"Corporate titles\" and \"Computer\" categories, while \"Professional driver types\" show the most neutral sentiment.\\n- Benchmarks used to evaluate LLM safety may have limitations and may not adequately cover all demographic categories or adversarial inputs.\\n- It is important to monitor disaggregated metrics and benchmarks to better understand and analyze the behavior of LLMs across different demographic groups.',\n",
       "  ' \\n\\n- Study examines toxicity in model generations across various demographic groups\\n- Demographic groups include Asian, Mexican, Muslim, physical disability, Jewish, Middle Eastern, Chinese, mental disability, Latino, Native American, women, Black, and LGBTQ\\n- Models used include Pretrained MPT7B, 30B, Falcon7B, 40B, Llama 17B, 13B, 33B, 65B, Llama 27B, 13B, 34B, and 70B, as well as Fine-tuned ChatGPT, MPT-instruct 7B, Falcon-instruct 7B, Llama 2-Chat7B, 13B, 34B, and 70B\\n- Results show low toxicity in model generations for most demographic groups, with some variations among models and groups\\n- Mean sentiment scores also show similar patterns, with low toxicity across most groups and models\\n- Overall, the study highlights the importance of considering diversity and inclusivity in natural language processing models.',\n",
       "  ' \\n\\n- Sentiment scores were collected for American actors and actresses using various pretrained and fine-tuned models.\\n- Existing benchmarks may not fully evaluate the ability of chat models to maintain context and avoid generating toxic content.\\n- The BOLD dataset uses prompts extracted from Wikipedia and has six to nine words.\\n- Safety in chat models involves user experience and long-term effects, which may not be captured by benchmarks alone.\\n- Human annotators were used to collect annotations for the supervised fine-tuning stage and human preferences to train reward models.\\n- Annotators were instructed to prioritize harmlessness over informativeness and helpfulness in their responses.\\n- Categories of responses that could lead to negative user experiences were shared with annotators.',\n",
       "  ' \\n\\n• Pretrained models show similar sentiment scores for all religions, with Sikhism having the lowest scores and Judaism having the highest.\\n• Fine-tuned models show higher sentiment scores for all religions, with Buddhism and Sikhism having the highest scores.\\n• Pretrained models show varying sentiment scores for different political ideologies, with liberalism and democracy having the highest scores and anarchism and fascism having the lowest.\\n• Fine-tuned models show higher sentiment scores for all political ideologies, with liberalism and democracy having the highest scores and anarchism and fascism having the lowest.\\n• Overall, fine-tuned models show higher sentiment scores for both religious and political ideologies compared to pretrained models.',\n",
       "  '\\n- Metalworking, sewing, healthcare, computer, film & television, artistic, scientific, entertainer, dance, nursing specialties, writing, professional driver types, engineering branches, mental health, theatre personnel, corporate titles, industrial, railway industry are all professions included in the data.\\n- The data is divided into different groups, with each group having a mean sentiment score.\\n- Negative user experience categories were identified and avoided in the annotations.\\n- A quality assurance process was implemented to ensure high quality annotations.\\n- Annotators were selected through a multi-step assessment process.',\n",
       "  '\\n\\n- Multi-step assessment process to select annotators for data collection tasks\\n- Tests included: understanding of guidelines, alignment with quality assessment criteria, alignment with sensitive topics guidelines, reading and writing skills\\n- First test: 3 sections (grammar, reading comprehension, writing style), 50 minutes, must score 90% on part I and average of 4 on parts II and III to pass\\n- Second test: 42 questions on sensitive topics alignment, answer ranking, and written examples, must agree with criteria on 80% of answers and score 4 out of 5 on written examples to pass.',\n",
       "  '\\n• Third test measured alignment with quality assessment criteria through 31 questions.\\n• Annotators who agreed with preferences in more than 26 questions passed the test.\\n• Last test assessed prompt-response by choosing a minimum of 6 out of 18 prompts to write responses for.\\n• Annotators who scored an average of >4 passed the training.\\n• Dataset contamination is a concern due to publicly available training data.\\n• Previous methodologies used n-gram collision detection to identify contaminated samples.\\n• New methodology considers contamination in tokenized input and from a bottom-up perspective.\\n• A \"skipgram budget\" of four tokens is allowed to account for variations in sample format.\\n• Suffix arrays are used to identify 10(+)-skipgrams in the dataset.\\n• Confounding factors make it difficult to determine if dataset contamination contributes to evaluation performance.',\n",
       "  '\\n• Dataset contamination affects evaluation performance.\\n• \"Cleanest\" examples are expected to have a worse average score than their complement.\\n• \"Dirtiest\" examples are expected to have a better average score than their complement.\\n• Insufficient evidence for contamination if only one of these is true.\\n• Four subset types defined: \"Clean\", \"Not clean\", \"Not dirty\", and \"Dirty\" samples.\\n• \"Clean\" samples have less than 20% token contamination.\\n• \"Not clean\" samples have greater than or equal to 20% token contamination.\\n• \"Not dirty\" samples have less than 80% token contamination.\\n• \"Dirty\" samples have greater than or equal to 80% token contamination.\\n• An additional confounding factor is addressed directly.\\n• Possibility of sample contamination with given definition.',\n",
       "  ' \\n- Contamination analysis results for affected datasets \\n- Avg. Contam. % denotes the average per-sample contamination percentage for the given subset type \\n- Models sizes refer to pretrained-only models \\n- No other evaluation datasets had sufficient evidence to be considered affected by contamination \\n- Zn=(¯X−µn)/σn, where n is the size of the sample subset type \\n- By the Central Limit Theorem, Zntendstowardsastandardnormaldistribution \\n- Only HellaSwag and MMLU-Humanities appear to have been boosted due to contamination in the training data \\n- The 70B model appears to have gained a greater benefit than the 7B model \\n- The impact of this effect on MMLU-Humanities appears to cause a benefit for MMLU-Overall for the 70B model \\n- No other dataset (for any choice of L) appears to have benefitted from dataset contamination \\n- Omit results from these datasets for conciseness',\n",
       "  \" \\n- Model card for Llama 2, a language model developed by Meta AI\\n- Comes in 3 parameter sizes and variations for input and output models\\n- Trained between January 2023 and July 2023\\n- Intended for commercial and research use in English, with potential for natural language generation tasks\\n- Pretrained on 2 trillion tokens of publicly available data, with additional fine-tuning data from instruction datasets and human-annotated examples\\n- Utilized custom training libraries and third-party cloud compute for training and evaluation\\n- Estimated carbon footprint of 539 tCO2eq, offset by Meta's sustainability program\\n- Potential for inaccurate or objectionable responses, developers should perform safety testing and tuning before deployment.\"],\n",
       " 'output_text': '\\n\\n- Llama 2-Chat is a collection of pretrained and fine-tuned large language models optimized for dialogue use cases.\\n- It outperforms open-source chat models on most benchmarks and has been evaluated for helpfulness and safety.\\n- The authors provide a detailed description of their approach to fine-tuning and safety improvements.\\n- The goal is to enable the community to build on their work and contribute to the responsible development of LLMs.\\n- The paper discusses the pretraining, fine-tuning, and safety measures used in the development of Llama 2 and Llama 2-Chat.\\n- The models were evaluated on various benchmarks and showed strong performance.\\n- Safety measures were implemented in both the pretraining and fine-tuning stages.\\n- The paper also shares observations and insights from the development process.\\n- A responsible release strategy and code examples are provided for safe deployment.\\n- The study compares two reinforcement learning algorithms and explores the use of Ghost Attention for dialogue control over multiple turns.\\n- The final system message for training is constructed by randomly combining synthetic constraints.\\n- Llama 2-Chat has been evaluated for helpfulness and safety, with high inter-rater reliability and low overall violation percentage.\\n- Researchers have also studied moral self-correction in large language models'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['intermediate_steps', 'output_text'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sarjana-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
